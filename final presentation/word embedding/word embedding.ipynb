{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b923a1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "import jieba\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95bd12bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "11549946",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_file = \"./cn_stopwords.txt\"\n",
    "with open(stopwords_file, \"r\", encoding=\"utf-8\") as file:\n",
    "    stop_words_list = file.readlines()\n",
    "stop_words_list = [line.strip() for line in stop_words_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5300027c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./骆驼祥子.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "text = re.sub(r'[^\\w\\s]', '', text) \n",
    "text = re.sub(r'\\s+', ' ', text)\n",
    "text = text.replace(\" \", \"\")\n",
    "\n",
    "raw_text_0 = list(jieba.cut(text, cut_all=False))\n",
    "raw_text = [word for word in raw_text_0 if word not in stop_words_list] \n",
    "\n",
    "vocab = set(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db1ca370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['祥子', '骆驼', '祥子', '骆驼'], '介绍'), (['介绍', '祥子', '骆驼', '骆驼'], '祥子'), (['祥子', '介绍', '骆驼', '外号'], '骆驼'), (['骆驼', '祥子', '外号', '先'], '骆驼'), (['骆驼', '骆驼', '先', '说祥子'], '外号')]\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_SIZE = 2  # 2 words to the left, 2 to the right\n",
    "EMBEDDING_DIM = 50\n",
    "\n",
    "# By deriving a set from `raw_text`, we deduplicate the array\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "data = []\n",
    "for i in range(CONTEXT_SIZE, len(raw_text) - CONTEXT_SIZE):\n",
    "    context = (\n",
    "        [raw_text[i - j - 1] for j in range(CONTEXT_SIZE)]\n",
    "        + [raw_text[i + j + 1] for j in range(CONTEXT_SIZE)]\n",
    "    )\n",
    "    target = raw_text[i]\n",
    "    data.append((context, target))\n",
    "print(data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e3b2967a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim).to(device)\n",
    "        self.linear1 = nn.Linear(embedding_dim, 128).to(device)\n",
    "        self.linear2 = nn.Linear(128, vocab_size).to(device)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = torch.mean(self.embeddings(inputs), dim=0).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs\n",
    "\n",
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = CBOW(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "afa095b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[383040.6752166748, 371139.4347035885, 349080.9028342962, 342351.5752040148, 339795.038377285, 337973.7778482735, 336484.98399430513, 335140.70063331723, 333851.7763360292, 332568.11263199896, 331258.9643229209, 329905.11920079216, 328492.62182867713]\n",
      "运行时间: 820.6160748004913 \n"
     ]
    }
   ],
   "source": [
    "import time  \n",
    "start_time = time.time()  \n",
    "\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for context, target in data:\n",
    "\n",
    "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words into integer indices and wrap them in tensors)\n",
    "        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long).to(device)\n",
    "\n",
    "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "        # new instance, you need to zero out the gradients from the old instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 3. Run the forward pass, getting log probabilities over next words\n",
    "        log_probs = model(context_idxs)\n",
    "\n",
    "        # Step 4. Compute your loss function. (Again, Torch wants the target word wrapped in a tensor)\n",
    "        loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long).to(device))\n",
    "\n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "        total_loss += loss.item()\n",
    "    losses.append(total_loss)\n",
    "print(losses)  # The loss decreased every iteration over the training data!\n",
    "\n",
    "end_time = time.time()  \n",
    "print(f\"运行时间: {end_time - start_time} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a701d147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 764, 2245,  764, 2245])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create your model and train. Here are some functions to help you make the data ready for use by your module.\n",
    "def make_context_vector(context, word_to_ix):\n",
    "    idxs = [word_to_ix[w] for w in context]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "make_context_vector(data[0][0], word_to_ix)  # example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22b5190",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62278746",
   "metadata": {},
   "source": [
    "## 使用gensim中的Word2Vec包实现CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5d43f9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ce76c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- sentences: 可迭代的语句列表，较大的语料库可以考虑从磁盘/IO的形式传输\n",
    "- vector_size: 单词向量的维数\n",
    "- window: 句子中当前单词与预测单词的最大距离\n",
    "- min_count: 忽略总频率低于此值的所有单词\n",
    "- workers: 使用多个 worker 线程训练模型\n",
    "- sg: 训练算法，1-> skip-gram 否则 -> CBOW\n",
    "- hs: 1 -> 分层 softmax 方法，否则 -> 负采样\n",
    "- negative: >0 则使用负采样，通常推荐距离为 [5-20]，如果设置为0则不适用负采样\n",
    "- alpha: 初始学习率\n",
    "- min_alpha: 随着训练进行，学习率将线性下降至 min_alpha\n",
    "- max_vocab_size: 词库限制，每 1000w 个字类型大约需要1GB的 RAM\n",
    "- sample: 配置较高频率的单词随机下采样的阈值，生效范围 (0,1e-5)\n",
    "- epochs 迭代次数\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d087a97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['三国演义'], ['第一回', '宴', '桃园', '豪杰', '三', '结义', '斩', '黄巾', '英雄', '首', '立功'], ['滚滚', '长江', '东', '逝水', '浪花', '淘尽', '英雄', '是非成败', '转头', '空'], ['青山', '依旧', '在', '几度', '夕阳红', '白发', '渔樵', '江渚上', '惯'], ['看', '秋月春风', '一壶', '浊酒', '喜相逢', '古今', '多少', '事', '都', '付']]\n"
     ]
    }
   ],
   "source": [
    "with open(\"三国演义.txt\", 'r',encoding='utf-8')as f: # 读入文本\n",
    "    lines = []\n",
    "    for line in f: #分别对每段分词\n",
    "        temp = jieba.lcut(line)  #结巴分词 精确模式\n",
    "        words = []\n",
    "        for i in temp:\n",
    "            #过滤掉所有的标点符号\n",
    "            i = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\'””《》]+|[+——！，。？、~@#￥%……&*（）：；‘]+\", \"\", i)\n",
    "            if len(i) > 0:\n",
    "                words.append(i)\n",
    "        if len(words) > 0:\n",
    "            lines.append(words)\n",
    "print(lines[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a548c155",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(\n",
    "    lines, \n",
    "    vector_size = 100, \n",
    "    hs = 1, \n",
    "    sg = 2, \n",
    "    min_count = 1, \n",
    "    window = 2,  \n",
    "    workers = 4, \n",
    "    epochs = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "62c53bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "孔明的词向量：\n",
      " [-0.32365045  0.27591166  0.5305709  -0.23610966 -0.40783113 -0.05334409\n",
      " -0.12471253  0.30638564  0.75083137 -0.64840454 -0.0165752   0.40281984\n",
      "  0.03974082 -0.30338097  0.0661559  -0.32427162 -0.24906923 -0.40847516\n",
      " -0.39091098  0.11982058 -0.14609097 -0.0076288   0.3399963   0.00193367\n",
      "  0.72057104  0.21825542 -0.9662531  -0.35575494 -0.26800779 -0.22006762\n",
      " -0.3623526  -0.06418754  0.0697577  -0.2002573  -0.0141564   0.702028\n",
      "  0.02826011  0.31054723  0.55619836 -0.38314077  0.50334597 -0.4325456\n",
      " -0.5552436  -0.50480545  0.12148239 -0.66760194 -0.63635737  0.40741077\n",
      "  0.39663538 -0.09630173 -0.24927041 -0.04164707 -0.17518984  0.7221643\n",
      "  0.3903633  -0.00678943  0.5938008  -0.26348865 -0.41011506 -0.40510067\n",
      " -0.39398313 -0.26716083  0.5274327   0.37557262  0.47788078  0.1622746\n",
      "  0.3886236   0.7362393  -0.31729952  0.41644067 -0.6646033   0.21656819\n",
      "  0.72482973  0.10144351  0.38675648 -0.02992108 -0.25849965 -0.15729703\n",
      " -0.58085984  0.19022861 -0.04304889 -0.44869766 -0.03818106  0.08713236\n",
      " -0.29699644  0.07344805  0.59316033  0.41191667 -0.12846035 -0.03103376\n",
      "  0.12596759 -0.10565289  0.05203108 -0.3798121  -0.15744796  0.07548639\n",
      "  0.02230603 -0.34300146  0.421915   -0.4369814 ]\n"
     ]
    }
   ],
   "source": [
    "print(\"孔明的词向量：\\n\",model.wv.get_vector('孔明'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "78c3a49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "和孔明相关性最高的前20个词语：\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('玄德', 0.7855352759361267),\n",
       " ('孟获', 0.7491429448127747),\n",
       " ('先主', 0.7122588157653809),\n",
       " ('懿', 0.71099853515625),\n",
       " ('周瑜', 0.7088468074798584),\n",
       " ('鲁肃', 0.7035942673683167),\n",
       " ('姜维', 0.6889432072639465),\n",
       " ('孙夫人', 0.686983048915863),\n",
       " ('郝昭', 0.6833346486091614),\n",
       " ('心中', 0.6751553416252136),\n",
       " ('孔明来', 0.6660807728767395),\n",
       " ('瑜', 0.662392258644104),\n",
       " ('庞统', 0.6611930727958679),\n",
       " ('孙权', 0.6606889367103577),\n",
       " ('后主', 0.6582843661308289),\n",
       " ('关公', 0.6560310125350952),\n",
       " ('王允', 0.6555484533309937),\n",
       " ('马谡', 0.6552057862281799),\n",
       " ('孔明回', 0.6539878845214844),\n",
       " ('玄德公', 0.6519277095794678)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n和孔明相关性最高的前20个词语：\")\n",
    "model.wv.most_similar('孔明', topn = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b8b3562f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSimilarSeq(key, model, top = 10):\n",
    "    print(\"Top For %s ======================\" % key)\n",
    "    sims = model.wv.most_similar(key, topn = top)\n",
    "    for i in sims:\n",
    "        print(i)\n",
    "    print(\"End Sim For %s ======================\" % key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ab59ba03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top For 玄德 ======================\n",
      "('孔明', 0.7855352163314819)\n",
      "('云长', 0.7299472689628601)\n",
      "('孙夫人', 0.7004892826080322)\n",
      "('鲁肃', 0.698726236820221)\n",
      "('孔明遂', 0.6961550712585449)\n",
      "('孟获', 0.6815526485443115)\n",
      "('庞统', 0.6662197113037109)\n",
      "('诸葛均', 0.6655987501144409)\n",
      "('孙权', 0.6647616624832153)\n",
      "('孙乾', 0.6642411351203918)\n",
      "End Sim For 玄德 ======================\n",
      "Top For 云长 ======================\n",
      "('张飞', 0.7685154676437378)\n",
      "('赵云', 0.7665302753448486)\n",
      "('关公', 0.7517321109771729)\n",
      "('岱', 0.7360236644744873)\n",
      "('玄德', 0.7299474477767944)\n",
      "('孙夫人', 0.7195055484771729)\n",
      "('王忠', 0.7171892523765564)\n",
      "('魏延', 0.7167795896530151)\n",
      "('翼德', 0.7129519581794739)\n",
      "('忠', 0.7106291055679321)\n",
      "End Sim For 云长 ======================\n"
     ]
    }
   ],
   "source": [
    "getSimilarSeq(\"玄德\", model)\n",
    "getSimilarSeq(\"云长\", model) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
